{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9d2cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import shap \n",
    "import pickle\n",
    "import sklearn \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from types import SimpleNamespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a8aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dd_package.data.dyslexia_data import DyslexiaData\n",
    "from dd_package.data.preprocess import preprocess_data\n",
    "\n",
    "from dd_package.models.regression_estimators import RegressionEstimators\n",
    "\n",
    "from dd_package.common.utils import save_a_dict, load_a_dict, print_the_evaluated_results\n",
    "\n",
    "from dd_package.data.preprocess import minmax_standardizer_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e0c1801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "configs = {\n",
    "    \"models_path\": Path(\"/home/soroosh/Programmes/DD/Models\"),\n",
    "    \"results_path\": Path(\"/home/soroosh/Programmes/DD/Results\"),\n",
    "    \"figures_path\": Path(\"/home/soroosh/Programmes/DD/Figures\"),\n",
    "    \"params_path\": Path(\"/home/soroosh/Programmes/DD//Params\"),\n",
    "    \"n_repeats\": 10,\n",
    "    \"n_splits\": 5,\n",
    "}\n",
    "\n",
    "configs = SimpleNamespace(**configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bf7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_name=\"dd_fix_demo\"\n",
    "\n",
    "estimator_name = \"mlp_cls\"\n",
    "to_shuffle = True\n",
    "to_exclude_at_risk = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27a1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "specifier = data_name + \"-\" + estimator_name + \\\n",
    "\"--shuffled:\" + str(to_shuffle) + \\\n",
    "\"--exclude at risk:\" + str(to_exclude_at_risk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a10bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs.data_name = data_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0c9c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pp = \"mm-spec\" \n",
    "\n",
    "if pp == \"mm-spec\":\n",
    "    specifier = specifier + \"--pp: \" + pp\n",
    "\n",
    "    \n",
    "    \n",
    "configs.specifier = specifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a569c1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dd_fix_demo-mlp_cls--shuffled:True--exclude at risk:0--pp: mm-spec--pp: mm-spec'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "configs.specifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a334f200",
   "metadata": {},
   "source": [
    "# demo-Fcls-mm:\n",
    "\n",
    "- Chosen model: **MLP**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f70fb3",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b8b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dd = DyslexiaData(path=\"../../datasets/\", n_repeats=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae3f4789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Demo data: \n",
      "  dyslexia (72, 9)\n",
      "  norm (213, 9)\n",
      "  risk (22, 9)\n",
      " \n",
      "Loading Fixation report data:\n",
      "  dyslexia (59770, 7)\n",
      "  norm (139507, 7)\n",
      "  risk (26073, 7)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if data_name == \"dd_demo\":\n",
    "    # dict of dicts, s.t each dict contains pd.df of a class, e.g normal\n",
    "    _ = dd.get_demo_datasets()  # demos and phonological (which is initially part of demo)\n",
    "    demo_phono = dd.concat_classes_demo()\n",
    "\n",
    "    # The optimize way to exclude at-risk class\n",
    "    if to_exclude_at_risk == 1:\n",
    "        to_exclude_at_risk = True\n",
    "        demo_phono = demo_phono.loc[demo_phono.Group != 2]\n",
    "\n",
    "    df_data_to_use = demo_phono.loc[:, [\n",
    "                                           'Group', 'SubjectID', 'Sex', 'Grade', 'Age', 'IQ', 'Reading_speed',\n",
    "                                       ]]\n",
    "    c_features = ['Sex', 'Grade', ]\n",
    "    indicators = ['SubjectID', ]\n",
    "    targets = [\"Group\", \"Reading_speed\", ]\n",
    "\n",
    "elif data_name == \"dd_fix_demo\":\n",
    "    # dict of dicts, s.t each dict contains pd.df of a class, e.g normal\n",
    "    _ = dd.get_demo_datasets()  # demos\n",
    "    _ = dd.get_fix_datasets()  # fixes\n",
    "\n",
    "    # concatenate pd.dfs to a pd.df\n",
    "    fix = dd.concat_classes_fix()\n",
    "    demo_phono = dd.concat_classes_demo()\n",
    "\n",
    "    # The optimize way to exclude at-risk class\n",
    "    if to_exclude_at_risk == 1:\n",
    "        to_exclude_at_risk = True\n",
    "        fix = fix.loc[fix.Group != 2]\n",
    "        demo_phono = demo_phono.loc[demo_phono.Group != 2]\n",
    "\n",
    "    fix_demo = dd.concat_dfs(\n",
    "        df1=fix,\n",
    "        df2=demo_phono,\n",
    "        features1=fix.columns,\n",
    "        features2=[\n",
    "            \"Sex\", \"Grade\", \"Age\", \"IQ\", \"Reading_speed\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df_data_to_use = fix_demo\n",
    "    c_features = ['Sex', 'Grade', ]\n",
    "    indicators = [\n",
    "        'SubjectID', 'Sentence_ID', 'Word_Number',\n",
    "    ]\n",
    "\n",
    "    targets = [\"Group\", \"Reading_speed\", ]\n",
    "    \n",
    "elif data_name == \"dd_fix\":\n",
    "\n",
    "    # dict of dicts, s.t each dict contains pd.df of a class, e.g normal\n",
    "    _ = dd.get_fix_datasets()  # fixes\n",
    "    # concatenate pd.dfs to a pd.df\n",
    "    fix = dd.concat_classes_fix()\n",
    "\n",
    "    # The optimize way to exclude at-risk class\n",
    "    if to_exclude_at_risk == 1:\n",
    "        to_exclude_at_risk = True\n",
    "        fix = fix.loc[fix.Group != 2]\n",
    "\n",
    "    df_data_to_use = fix\n",
    "    c_features = None\n",
    "    indicators = [\n",
    "        'SubjectID', 'Sentence_ID', 'Word_Number',\n",
    "    ]\n",
    "\n",
    "    targets = [\"Group\", ]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c67800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>SubjectID</th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Word_Number</th>\n",
       "      <th>FIX_X</th>\n",
       "      <th>FIX_Y</th>\n",
       "      <th>FIX_DURATION</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Grade</th>\n",
       "      <th>Age</th>\n",
       "      <th>IQ</th>\n",
       "      <th>Reading_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>177.3</td>\n",
       "      <td>531.8</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>200.5</td>\n",
       "      <td>545.2</td>\n",
       "      <td>247.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>282.7</td>\n",
       "      <td>542.4</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>358.1</td>\n",
       "      <td>545.5</td>\n",
       "      <td>268.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>392.8</td>\n",
       "      <td>536.3</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group SubjectID  Sentence_ID  Word_Number  FIX_X  FIX_Y  FIX_DURATION  Sex  \\\n",
       "0      1        10            4            1  177.3  531.8         219.0    1   \n",
       "1      1        10            4            2  200.5  545.2         247.0    1   \n",
       "2      1        10            4            3  282.7  542.4         261.0    1   \n",
       "3      1        10            4            4  358.1  545.5         268.0    1   \n",
       "4      1        10            4            4  392.8  536.3         129.0    1   \n",
       "\n",
       "   Grade   Age    IQ  Reading_speed  \n",
       "0      4  10.0  23.0          102.0  \n",
       "1      4  10.0  23.0          102.0  \n",
       "2      4  10.0  23.0          102.0  \n",
       "3      4  10.0  23.0          102.0  \n",
       "4      4  10.0  23.0          102.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_data_to_use.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efa773a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_org, y_org = dd.get_onehot_features_targets(\n",
    "    data=df_data_to_use,\n",
    "    c_features=c_features,\n",
    "    indicators=indicators,\n",
    "    targets=targets,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ee1201d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grade_1</th>\n",
       "      <th>Grade_4</th>\n",
       "      <th>Age</th>\n",
       "      <th>Grade_3</th>\n",
       "      <th>IQ</th>\n",
       "      <th>Grade_6</th>\n",
       "      <th>FIX_Y</th>\n",
       "      <th>FIX_X</th>\n",
       "      <th>Sex_1</th>\n",
       "      <th>FIX_DURATION</th>\n",
       "      <th>Grade_5</th>\n",
       "      <th>Grade_2</th>\n",
       "      <th>Sex_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>531.8</td>\n",
       "      <td>177.3</td>\n",
       "      <td>1</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>545.2</td>\n",
       "      <td>200.5</td>\n",
       "      <td>1</td>\n",
       "      <td>247.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>542.4</td>\n",
       "      <td>282.7</td>\n",
       "      <td>1</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>545.5</td>\n",
       "      <td>358.1</td>\n",
       "      <td>1</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>536.3</td>\n",
       "      <td>392.8</td>\n",
       "      <td>1</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Grade_1  Grade_4   Age  Grade_3    IQ  Grade_6  FIX_Y  FIX_X  Sex_1  \\\n",
       "0        0        1  10.0        0  23.0        0  531.8  177.3      1   \n",
       "1        0        1  10.0        0  23.0        0  545.2  200.5      1   \n",
       "2        0        1  10.0        0  23.0        0  542.4  282.7      1   \n",
       "3        0        1  10.0        0  23.0        0  545.5  358.1      1   \n",
       "4        0        1  10.0        0  23.0        0  536.3  392.8      1   \n",
       "\n",
       "   FIX_DURATION  Grade_5  Grade_2  Sex_2  \n",
       "0         219.0        0        0      0  \n",
       "1         247.0        0        0      0  \n",
       "2         261.0        0        0      0  \n",
       "3         268.0        0        0      0  \n",
       "4         129.0        0        0      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_org.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4beffc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Reading_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group  Reading_speed\n",
       "0      1          102.0\n",
       "1      1          102.0\n",
       "2      1          102.0\n",
       "3      1          102.0\n",
       "4      1          102.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_org.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ab4345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_method: classification\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "estimator_name = \"MLP_cls\"\n",
    "\n",
    "if estimator_name.split(\"_\")[-1] == \"reg\":\n",
    "    learning_method = \"regression\"\n",
    "    y = y_org.Reading_speed.values\n",
    "\n",
    "elif estimator_name.split(\"_\")[-1] == \"cls\":\n",
    "    learning_method = \"classification\"\n",
    "    y = y_org.Group.values\n",
    "\n",
    "elif estimator_name.split(\"_\")[-1] == \"clu\":\n",
    "    from dd_package.models.clustering_estimators import ClusteringEstimators\n",
    "    y = y_org.Group.values\n",
    "else:\n",
    "    assert False, \"Undefined algorithm and thus undefined target values\"\n",
    "    \n",
    "print(\"learning_method:\", learning_method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25adc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x = preprocess_data(x=x_org, pp=\"mm\")  # only x is standardized\n",
    "\n",
    "\n",
    "x_df = dd.get_preprocessed_features(x_dum=x_org)\n",
    "x = x_df.values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7940108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Grade_1</th>\n",
       "      <th>Grade_4</th>\n",
       "      <th>Grade_3</th>\n",
       "      <th>Grade_6</th>\n",
       "      <th>Grade_5</th>\n",
       "      <th>Grade_2</th>\n",
       "      <th>IQ</th>\n",
       "      <th>FIX_Y</th>\n",
       "      <th>FIX_X</th>\n",
       "      <th>FIX_DURATION</th>\n",
       "      <th>Sex_1</th>\n",
       "      <th>Sex_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.503379</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.022293</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.536921</td>\n",
       "      <td>0.060366</td>\n",
       "      <td>0.025156</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.529912</td>\n",
       "      <td>0.121250</td>\n",
       "      <td>0.026588</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.537672</td>\n",
       "      <td>0.177098</td>\n",
       "      <td>0.027303</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.514643</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.013089</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age   Grade_1   Grade_4   Grade_3   Grade_6   Grade_5   Grade_2        IQ  \\\n",
       "0  0.5 -0.166667  0.833333 -0.166667 -0.166667 -0.166667 -0.166667  0.380952   \n",
       "1  0.5 -0.166667  0.833333 -0.166667 -0.166667 -0.166667 -0.166667  0.380952   \n",
       "2  0.5 -0.166667  0.833333 -0.166667 -0.166667 -0.166667 -0.166667  0.380952   \n",
       "3  0.5 -0.166667  0.833333 -0.166667 -0.166667 -0.166667 -0.166667  0.380952   \n",
       "4  0.5 -0.166667  0.833333 -0.166667 -0.166667 -0.166667 -0.166667  0.380952   \n",
       "\n",
       "      FIX_Y     FIX_X  FIX_DURATION  Sex_1  Sex_2  \n",
       "0  0.503379  0.043182      0.022293    0.5   -0.5  \n",
       "1  0.536921  0.060366      0.025156    0.5   -0.5  \n",
       "2  0.529912  0.121250      0.026588    0.5   -0.5  \n",
       "3  0.537672  0.177098      0.027303    0.5   -0.5  \n",
       "4  0.514643  0.202800      0.013089    0.5   -0.5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc3d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = dd.get_stratified_train_test_splits(\n",
    "        x=x, y=y,\n",
    "        labels=y_org.Group.values,\n",
    "        to_shuffle=to_shuffle,\n",
    "        n_splits=10\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb0a9117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(collections.defaultdict,\n",
       "            {'1': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.013,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.008,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.037, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.026, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.018, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '2': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.02 , -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.005,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.004,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.004,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.023, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.016, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '3': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.004,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.014,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.028,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.037, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.021, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.017, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '4': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.02 ,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.048,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.036, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.012, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.035, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '5': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.02 , -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.015,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.018,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.006,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.025, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.027, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '6': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.003,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.04 ,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.008, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.016, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.02 , -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '7': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.014,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.005,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.01 ,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.036, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.047, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.027, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '8': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.02 , -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.02 ,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.024,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.016,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.037, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.02 , -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '9': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.013,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.014,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.017, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.058, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.033, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])}),\n",
       "             '10': defaultdict(list,\n",
       "                         {'x_train': array([[ 0.5  , -0.167,  0.833, ...,  0.022,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.025,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.027,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.022, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.019, -0.5  ,  0.5  ]]),\n",
       "                          'x_test': array([[ 0.5  , -0.167,  0.833, ...,  0.035,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.026,  0.5  , -0.5  ],\n",
       "                                 [ 0.5  , -0.167,  0.833, ...,  0.011,  0.5  , -0.5  ],\n",
       "                                 ...,\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.007, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.015, -0.5  ,  0.5  ],\n",
       "                                 [ 0.5  , -0.167, -0.167, ...,  0.028, -0.5  ,  0.5  ]]),\n",
       "                          'y_train': array([1, 1, 1, ..., 3, 3, 3]),\n",
       "                          'y_test': array([1, 1, 1, ..., 3, 3, 3])})})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f918b5f5",
   "metadata": {},
   "source": [
    "## Load the tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e9b413d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/soroosh/Programmes/DD/Params/dd_fix_demo-mlp_cls--shuffled:True--exclude at risk:0--pp: mm-spec--pp: mm-spec.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_596760/3761298246.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tuned_params = load_a_dict(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         )\n\u001b[1;32m      5\u001b[0m \u001b[0mtuned_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programmes/DD/DD_package/notebooks/../dd_package/common/utils.py\u001b[0m in \u001b[0;36mload_a_dict\u001b[0;34m(name, save_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_a_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0ma_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/soroosh/Programmes/DD/Params/dd_fix_demo-mlp_cls--shuffled:True--exclude at risk:0--pp: mm-spec--pp: mm-spec.pickle'"
     ]
    }
   ],
   "source": [
    "\n",
    "tuned_params = load_a_dict(\n",
    "    name=configs.specifier,\n",
    "    save_path=configs.params_path\n",
    "        )\n",
    "tuned_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae735f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "specifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d98b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_a_dict(a_dict=tuned_params,\n",
    "#             name='dd_fix_demo-mlp_reg--shuffled:True--exclude at risk:0',\n",
    "#             save_path=configs.params_path,\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abedb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdl = sklearn.neural_network.MLPClassifier(\n",
    "    activation=tuned_params[\"activation\"], \n",
    "    hidden_layer_sizes=tuned_params[\"hidden_layer_sizes\"],\n",
    "    max_iter=tuned_params[\"max_iter\"],\n",
    "    solver=tuned_params[\"solver\"],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mdl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e356bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_train = data[\"6\"][\"x_train\"]\n",
    "tmp_x_test = data[\"6\"][\"x_test\"]\n",
    "y_train = data[\"6\"][\"y_train\"]\n",
    "tmp_y_test = data[\"6\"][\"y_test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0116ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class_names = ['Norm', 'At-risk', 'Dyslexic']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35debec8",
   "metadata": {},
   "source": [
    "## Previously obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d19bfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "results = load_a_dict(\n",
    "    name=configs.specifier,\n",
    "    save_path=configs.results_path,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, 11):\n",
    "    i = str(i)\n",
    "    y_test_ = results[i][\"y_test\"]\n",
    "    y_pred_ = results[i][\"y_pred\"]\n",
    "    y_prob_ = results[i][\"y_pred_prob\"]\n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(\n",
    "    y_true=y_test_, \n",
    "    y_pred=y_pred_\n",
    "    )\n",
    "    \n",
    "    disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=class_names\n",
    "                             )\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print(\n",
    "        \"Kappa:\", sklearn.metrics.cohen_kappa_score(y1=y_test_, y2=y_pred_),\n",
    "        \n",
    "        \"AUC ROC:\", sklearn.metrics.roc_auc_score(y_test_, y_prob_,\n",
    "                                                  multi_class=\"ovr\",\n",
    "                                                  average=\"weighted\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ac74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print_the_evaluated_results(\n",
    "    results=results, \n",
    "    learning_method=learning_method\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e7e1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mdl.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e539a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"model fitted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc3826",
   "metadata": {},
   "source": [
    "## Load new test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5650e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_sets_xls = pd.ExcelFile(\"../../datasets/Fix_Demo_test_data.xlsx\")\n",
    "print(data_sets_xls.sheet_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Loading Demo data: \")\n",
    "tmp_demo = pd.read_excel(data_sets_xls, 'demography_test',)\n",
    "tmp_demo = dd._remove_missing_data(df=tmp_demo)\n",
    "\n",
    "\n",
    "tmp_demo.replace(\n",
    "    to_replace={\"Sex\": {\"fem\": 1, \"f\": 1, \"masc\": 2, \"m\": 2}},\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "\n",
    "tmp_demo = tmp_demo.astype({\n",
    "    \"SubjectID\": str,\n",
    "    \"Sex\": int,\n",
    "    \"Grade\": int,\n",
    "    \"Age\": int,\n",
    "    \"IQ\": int,\n",
    "})\n",
    "\n",
    "\n",
    "demo_new = tmp_demo.sort_values(by=[\"SubjectID\"]).dropna()\n",
    "\n",
    "print(\" \", demo_new.shape)\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "demo_new.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"Loading Fix data: \")\n",
    "tmp_fix = pd.read_excel(data_sets_xls, 'fixations_test',)\n",
    "tmp_fix = dd._remove_missing_data(df=tmp_fix)\n",
    "\n",
    "\n",
    "tmp_fix.astype({\n",
    "    \"SubjectID\": str,\n",
    "    \"Sentence_ID\": int,\n",
    "    \"Word_Number\": int,\n",
    "    \"FIX_X\": float,\n",
    "    \"FIX_Y\": float,\n",
    "    \"FIX_DURATION\": float,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "fix_new = tmp_fix.sort_values(by=[\"SubjectID\"]).dropna()\n",
    "\n",
    "print(\" \", fix_new.shape)\n",
    "print(\" \")\n",
    "\n",
    "\n",
    "fix_new.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fix_demo_new = dd.concat_dfs(\n",
    "            df1=fix_new,\n",
    "            df2=demo_new,\n",
    "            features1=fix_new.columns,\n",
    "            features2=[\n",
    "                \"Sex\", \"Grade\", \"Age\", \"IQ\",\n",
    "            ],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93366b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "set(fix_demo_new.Grade), set(fix_demo.Grade)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27769edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set(fix_demo_new.Sex), set(fix_demo.Sex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb35d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fix_demo_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Artificially add two rows with grade 5 and grade 6 to handle missing values during dummy conversion\n",
    "\n",
    "fix_demo_new.loc[-1] = ['ext-G5', 1, 1, 123, 123, 123, 1, 5, 123, 123]\n",
    "fix_demo_new.index = fix_demo_new.index + 1  # shifting index\n",
    "\n",
    "fix_demo_new.loc[-2] = ['ext-G6', 1, 1, 123, 123, 123, 1, 6, 123, 123]\n",
    "fix_demo_new.index = fix_demo_new.index + 1  # shifting index\n",
    "\n",
    "fix_demo_new.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb97be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "indicators = [\"SubjectID\", \"Sentence_ID\", \"Word_Number\", ]\n",
    "\n",
    "fix_demo_new_dum = pd.get_dummies(data=fix_demo_new, columns=c_features)\n",
    "\n",
    "\n",
    "# Here we use x_org.columns to preserves the order of feature-space in train and test \n",
    "features = list(\n",
    "    set(x_org.columns).difference(\n",
    "        set(indicators).union(set(targets))\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fix_demo_new_dum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "features_ = list(\n",
    "    set(fix_demo_new.columns).difference(\n",
    "        set(indicators).union(set(targets))\n",
    "    )\n",
    ")\n",
    "\n",
    "features_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f1df4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove the the last two rows added synthetically to handle missing 5th and 6th grades.\n",
    "\n",
    "fix_demo_new_dum = fix_demo_new_dum.drop(fix_demo_new_dum.index[-2:])\n",
    "    \n",
    "fix_demo_new_dum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in range(len(features)):\n",
    "    print(f\"{features[f]} mean = {x[:, f].mean(): .3f} std= {x[:, f].std(): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df0eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818f7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# subject_ids = set(fix_demo_new_dum.SubjectID)\n",
    "# subject_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from ground truth\n",
    "subject_ids = [\n",
    "    \"nnr8\", \"nnr26\", \"nnr27\", \"nnr33\", \"nnr36\", \"nnr41\", \"nnr45\", \"nnr48\", \"nnr51\", \"nnr52\",\n",
    "    \"nnr58\", \"nnr61\", \"nnr66\", \"nnr67\", \"nnr70\", \"nnr73\", \"nnr74\", \"nnr82\", \"nnr84\", \"nnr85\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24620021",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ground truth \n",
    "\n",
    "ground_truth  = [\n",
    "    \"dyslexia\", \"dyslexia\", \"dyslexia\", \"dyslexia\", \"dyslexia\", \"dyslexia\", \"norm\", \"norm\", \n",
    "    \"norm\", \"norm\", \"dyslexia\", \"dyslexia\", \"dyslexia\", \"norm\", \"norm\", \"dyslexia\", \"norm\",\n",
    "    \"norm\", \"dyslexia\", \"norm\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aec212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels_true = []\n",
    "\n",
    "for g in ground_truth:\n",
    "\n",
    "    if g == \"norm\":\n",
    "        gg = 0    \n",
    "    elif g == \"at-risk\":\n",
    "        gg = 1\n",
    "    elif g == \"dyslexia\":\n",
    "        gg = 2\n",
    "        \n",
    "    labels_true.append(gg)\n",
    "    \n",
    "    \n",
    "labels_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x_org.Sex_1.mean(), x_org.Sex_2.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8df4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_subj.Sex_2.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113896be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa00267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = {}\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "        \n",
    "    x_test_subj = fix_demo_new_dum.loc[fix_demo_new_dum.SubjectID == subject_id]\n",
    "    x_test_subj_shuffle = x_test_subj.iloc[np.random.permutation(len(x_test_subj))]\n",
    "    \n",
    "    x_test_subj_pp = minmax_standardizer_(x_test=x_test_subj_shuffle.loc[:, features].values,\n",
    "                                         x_train=x_org.values)  \n",
    "    \n",
    "    print(\n",
    "        f\" Subject_id = {subject_id}: \\n\"\n",
    "        f\"  X_test {x_test_subj.shape}\" \n",
    "        f\" Shuffleed {x_test_subj_shuffle.shape}\"\n",
    "        f\" Preproccessed {x_test_subj_pp.shape} \"\n",
    "        f\" Equal feature space: {np.all(x_test_subj_shuffle.loc[:, features].columns ==  x_org.columns)} \\n\"\n",
    "    )\n",
    "    \n",
    "    predictions[subject_id] = {}\n",
    "    predictions[subject_id][\"Labels\"] = mdl.predict(x_test_subj_pp)\n",
    "    predictions[subject_id][\"Probs\"] = mdl.predict_proba(x_test_subj_pp)\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f45f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "thr = 85e-2\n",
    "labels_pred = []\n",
    "labels_prob = []\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    \n",
    "    ave_pred_probs = predictions[subject_id]['Probs'].mean(axis=0)\n",
    "    label = np.argmax(ave_pred_probs)\n",
    "    \n",
    "    if label == 0:\n",
    "        L = \"Normal\"\n",
    "        P = ave_pred_probs[0]\n",
    "        \n",
    "    elif label == 1:\n",
    "        L = \"At-risk\"\n",
    "        P = ave_pred_probs[1]\n",
    "        \n",
    "    elif label == 2:\n",
    "        L = \"Dyslexic\"\n",
    "        P = ave_pred_probs[2]\n",
    "    \n",
    "    print(\n",
    "        f\"Probability of subject ID {subject_id} being: \\n \"\n",
    "        f\"  Normal = {ave_pred_probs[0]: .3f}\"\n",
    "        f\", At-risk = {ave_pred_probs[1]: .3f}\"\n",
    "        f\", Dyslexic = {ave_pred_probs[2]: .3f}\"\n",
    "        f\" >> Assigned label: {L} \\n\" \n",
    ")\n",
    "    \n",
    "    if np.max(ave_pred_probs) < thr:\n",
    "        print(\n",
    "            \" ************************************* Attention ************************************* \\n \\n\"\n",
    "            f\" The model's predicted probability for subject ID {subject_id} of being {L} is {P:.3f}. \\n\"\n",
    "            f\" This probability is below our user-defined threshold ({thr:.3f}). \\n\"\n",
    "            f\" Therefore, an expert view is required. \\n \\n\"\n",
    "            \" ************************************************************************************* \\n \\n\"\n",
    "\n",
    "        )\n",
    "        \n",
    "    \n",
    "    labels_pred.append(label)\n",
    "    labels_prob.append(ave_pred_probs)\n",
    "\n",
    "\n",
    "    \n",
    "labels_prob = np.asarray(labels_prob)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac509e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10066541",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505a3f1",
   "metadata": {},
   "source": [
    "- SubID  True       Pred\n",
    "\n",
    "- nnr8\tdyslexia    Normal   Dyslexic\n",
    " \n",
    "- nnr26\tdyslexia    Normal   Dyslexic\n",
    "\n",
    "- nnr27\tdyslexia    Normal   Dyslexic\n",
    "\n",
    "- nnr33\tdyslexia    Normal   Normal\n",
    "\n",
    "- nnr36\tdyslexia    Dyslexic Normal\n",
    "\n",
    "- nnr41\tdyslexia    Dyslexic Dyslexic\n",
    "\n",
    "- nnr45\tnorm        Normal   Normal\n",
    "\n",
    "- nnr48\tnorm        Normal   Normal\n",
    "\n",
    "- nnr51\tnorm        Normal   Normal\n",
    "\n",
    "- nnr52\tnorm        Normal   Normal\n",
    "\n",
    "- nnr58\tdyslexia    Normal   Dyslexic\n",
    "\n",
    "- nnr61\tdyslexia    Dyslexic Dyslexic\n",
    "\n",
    "- nnr66\tdyslexia    Normal   Normal\n",
    "\n",
    "- nnr67\tnorm        Normal   Normal\n",
    " \n",
    "- nnr70\tnorm        Normal   Dyslexic\n",
    "\n",
    "- nnr73\tdyslexia    Dyslexic Dyslexic\n",
    "\n",
    "- nnr74\tnorm        Normal  Dyslexic\n",
    " \n",
    "- nnr82\tnorm        Normal  Normal\n",
    "\n",
    "- nnr84\tdyslexia    Normal  Dyslexic\n",
    "\n",
    "- nnr85\tnorm        Normal  Dyslexic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6716aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a800b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sklearn.metrics.confusion_matrix(\n",
    "    y_true=labels_true, \n",
    "    y_pred=labels_pred\n",
    ")\n",
    "\n",
    "\n",
    "fp = cm.sum(axis=0) - np.diag(cm)\n",
    "fn = cm.sum(axis=1) - np.diag(cm)\n",
    "tp = np.diag(cm)\n",
    "tn = cm.sum() - (fp + fn + tp)\n",
    "tnr = tn.astype(float) / (tn.astype(float) + fp.astype(float))\n",
    "_, support = np.unique(labels_true, return_counts=True)\n",
    "tnr = np.dot(tnr, support)/sum(support)\n",
    "\n",
    "\n",
    "\n",
    "auc_roc = sklearn.metrics.roc_auc_score(\n",
    "    labels_true, labels_pred,\n",
    "    multi_class=\"ovr\",\n",
    "    average=\"weighted\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pre = sklearn.metrics.precision_score(labels_true, labels_pred, average='weighted')\n",
    "rec = sklearn.metrics.recall_score(labels_true, labels_pred, average='weighted')\n",
    "\n",
    "fsc = sklearn.metrics.f1_score(labels_true, labels_pred, average='weighted')\n",
    "\n",
    "ari = sklearn.metrics.adjusted_rand_score(labels_true, labels_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71cf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                          display_labels=[\"normal\", \"dyslexic\"]\n",
    "                         )\n",
    "\n",
    "\n",
    "\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    f\" ari: {ari:.3f} \"\n",
    "    f\" presicion: {pre:.3f} \"\n",
    "    f\" recall:{rec:.3f} \"\n",
    "    f\" f1-score: {fsc:.3f} \"\n",
    "    f\" auc roc: {auc_roc: .3f} \"\n",
    "    f\" tnr: {tnr: .3f} \"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7496ac",
   "metadata": {},
   "source": [
    "# Hold on a second!\n",
    "\n",
    "\n",
    "**How good or bad these results are?**\n",
    "\n",
    "\n",
    "Let us see\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ee5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_ = dd.get_demo_datasets()  # demos and phonological (which is initially part of demo)\n",
    "demo_phono = dd.concat_classes_demo()\n",
    "\n",
    "# The optimize way to exclude at-risk class\n",
    "if to_exclude_at_risk == 1:\n",
    "    to_exclude_at_risk = True\n",
    "    demo_phono = demo_phono.loc[demo_phono.Group != 2]\n",
    "\n",
    "demo_old = demo_phono.loc[:, [\n",
    "    'Group', 'SubjectID', 'Sex', 'Grade', 'Age', 'IQ', 'Reading_speed',\n",
    "]]\n",
    "\n",
    "c_features = ['Sex', 'Grade', ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo_old_dys = demo_old.loc[demo_old.Group == 3]\n",
    "\n",
    "demo_old_dys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239d822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c193b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(19, 13))\n",
    "\n",
    "\n",
    "\n",
    "gender_old = [len(demo_old['Sex'].loc[demo_old['Sex']==f]) for f in set(demo_old['Sex'])]\n",
    "gender_old_dys = [len(demo_old_dys['Sex'].loc[demo_old_dys['Sex']==f]) for f in set(demo_old_dys['Sex'])]\n",
    "\n",
    "gender_new = [len(demo_new['Sex'].loc[demo_new['Sex']==f]) for f in set(demo_new['Sex'])]\n",
    "\n",
    "\n",
    "gender_new_dys  = [7, 4]\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 1)  # 4 datasets\n",
    "patches, texts, autotexts = ax.pie(\n",
    "    x=gender_old, \n",
    "    labels=['Female', 'Male'], \n",
    "    autopct='%.2f', \n",
    "    textprops={'fontsize': 14}                                   \n",
    ")\n",
    "    \n",
    "plt.title('Old Demo', fontsize=20)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 2)  # 4 datasets\n",
    "patches, texts, autotexts = ax.pie(\n",
    "    x=gender_new, \n",
    "    labels=['Female', 'Male'], \n",
    "    autopct='%.2f', \n",
    "    textprops={'fontsize': 14}                                   \n",
    ")\n",
    "\n",
    "\n",
    "plt.title('New Demo', fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 3)  # 4 datasets\n",
    "patches, texts, autotexts = ax.pie(\n",
    "    x=gender_old_dys, \n",
    "    labels=['Female', 'Male'], \n",
    "    autopct='%.2f', \n",
    "    textprops={'fontsize': 14}                                   \n",
    ")\n",
    "\n",
    "\n",
    "plt.title('Old Dyslexis', fontsize=20)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 4, 4)  # 4 datasets\n",
    "patches, texts, autotexts = ax.pie(\n",
    "    x=gender_new_dys, \n",
    "    labels=['Female', 'Male'], \n",
    "    autopct='%.2f', \n",
    "    textprops={'fontsize': 14}                                   \n",
    ")\n",
    "\n",
    "plt.title('New Dyslexis', fontsize=14)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dfdb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "old_demo_stat = [213, 22, 72, ]\n",
    "new_demo_stat = [9, 0, 11, ]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(19, 13))\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)  # 2 datasets\n",
    "patches, texts, autotexts = ax.pie(\n",
    "    x=old_demo_stat, \n",
    "    labels=['Normal', 'At-risk', 'Dyslexic'], \n",
    "    autopct='%.2f', \n",
    "    textprops={'fontsize': 14}                                   \n",
    ")\n",
    "    \n",
    "plt.title('Old Data', fontsize=20)\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)  # 2 datasets\n",
    "patches, texts, autotexts = ax.pie(\n",
    "    x=new_demo_stat, \n",
    "    labels=['Normal', 'At-risk', 'Dyslexic'], \n",
    "    autopct='%.2f', \n",
    "    textprops={'fontsize': 14}                                   \n",
    ")\n",
    "\n",
    "plt.title('New Data', fontsize=20)\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af046d3",
   "metadata": {},
   "source": [
    "# How should we be fair:\n",
    "\n",
    "- Random selection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sampling_ratio = (72*100)/213\n",
    "sampling_ratio ~ 33.33%\n",
    "\n",
    "3 normals 1 dyslexic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0612de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dyslexics_idx = np.array([0, 1, 2, 3, 4, 5, 10, 11, 12, 15, 18, ])\n",
    "\n",
    "\n",
    "dyslexics_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015eec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normal_idx = np.asarray([i for i in range(len(ground_truth)) if ground_truth[i]=='norm'])\n",
    "\n",
    "normal_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687a70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROC, PRE, REC, FSC, TNR, ARI = [], [], [], [], [], []\n",
    "\n",
    "\n",
    "for r in range(40000):\n",
    "    \n",
    "    d_idx = np.random.choice(dyslexics_idx, replace=True, size=3)\n",
    "    n_idx = np.random.choice(normal_idx, replace=True, size=9)\n",
    "    \n",
    "    idx = idx = np.concatenate([d_idx, n_idx])\n",
    "    labels_pred_sampled = [labels_pred[i] for i in idx]\n",
    "    labels_pred_prob_sampled = [np.max(labels_prob[i, [0, -1]]) for i in idx]\n",
    "    labels_true_sampled = [labels_true[i] for i in idx]\n",
    "    \n",
    "    labels_true_sampled_ = enc.fit_transform(np.asarray(labels_true_sampled).reshape(1, -1))\n",
    "    \n",
    "    \n",
    "    cm = sklearn.metrics.confusion_matrix(\n",
    "    y_true=labels_true_sampled, \n",
    "    y_pred=labels_pred_sampled\n",
    "    )\n",
    "\n",
    "\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    fn = cm.sum(axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tn = cm.sum() - (fp + fn + tp)\n",
    "    tnr = tn.astype(float) / (tn.astype(float) + fp.astype(float))\n",
    "    _, support = np.unique(labels_true_sampled, return_counts=True)\n",
    "    tnr = np.dot(tnr, support)/sum(support)\n",
    "\n",
    "\n",
    "\n",
    "    auc_roc = sklearn.metrics.roc_auc_score(\n",
    "        y_true=labels_true_sampled, \n",
    "        y_score=labels_pred_sampled,\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"weighted\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pre = sklearn.metrics.precision_score(\n",
    "        labels_true_sampled, labels_pred_sampled, average='weighted'\n",
    "    )\n",
    "    rec = sklearn.metrics.recall_score(\n",
    "        labels_true_sampled, labels_pred_sampled, average='weighted'\n",
    "    )\n",
    "\n",
    "    fsc = sklearn.metrics.f1_score(\n",
    "        labels_true_sampled, labels_pred_sampled, average='weighted'\n",
    "    )\n",
    "\n",
    "    ari = sklearn.metrics.adjusted_rand_score(\n",
    "        labels_true_sampled, labels_pred_sampled\n",
    "    )\n",
    "    \n",
    "\n",
    "    ROC.append(auc_roc)\n",
    "    PRE.append(pre)\n",
    "    REC.append(rec)\n",
    "    FSC.append(fsc)\n",
    "    TNR.append(tnr)\n",
    "    ARI.append(ari)\n",
    "    \n",
    "    if r % 505 == 0:\n",
    "        \n",
    "        disp = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=[\"normal\", \"dyslexic\"]\n",
    "                             )\n",
    "\n",
    "\n",
    "\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "\n",
    "        print(\n",
    "            f\" ari: {ari:.3f} \"\n",
    "            f\" presicion: {pre:.3f} \"\n",
    "            f\" recall:{rec:.3f} \"\n",
    "            f\" f1-score: {fsc:.3f} \"\n",
    "            f\" auc roc: {auc_roc: .3f} \"\n",
    "            f\" tnr: {tnr: .3f} \"\n",
    "        )\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "ROC = np.asarray(ROC)\n",
    "PRE = np.asarray(PRE)\n",
    "REC = np.asarray(REC)\n",
    "\n",
    "FSC = np.asarray(FSC)\n",
    "\n",
    "TNR = np.asarray(TNR)\n",
    "ARI = np.asarray(ARI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00747d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\n",
    "    f\" ari: {ARI.mean():.3f}  {ARI.std():.3f} \\n\"\n",
    "    f\" presicion: {PRE.mean():.3f}  {PRE.std():.3f} \\n\"\n",
    "    f\" recall: {REC.mean():.3f}  {REC.std():.3f} \\n\"\n",
    "    f\" f1-score: {FSC.mean():.3f}  {FSC.std():.3f} \\n\"\n",
    "    f\" auc roc: {ROC.mean(): .3f}  {ROC.std(): .3f} \\n\"\n",
    "    f\" tnr: {TNR.mean(): .3f}  {TNR.std(): .3f} \\n\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a26d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pred_prob_sampled = [labels_prob[i, [0, -1]] for i in idx]\n",
    "\n",
    "labels_pred_prob_sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3af3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40104020",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true_sampled_ = enc.fit_transform(np.asarray(labels_true_sampled).reshape(1, -1), )\n",
    "\n",
    "labels_true_sampled_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5821283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels_true_sampled\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08bc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e53bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_Jan2022",
   "language": "python",
   "name": "venv_jan2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
